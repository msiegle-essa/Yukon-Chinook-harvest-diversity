---
title: "Yukon Chinook Harvest/Diversity Tradeoffs"
output: html_document
---

# Yukon Chinook Harvest/Diversity Trade-offs 
##AYK-SSI #1701

## Project Memo: Fall 2018

```{r Load libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(grDevices)
library(plotrix)
library(R2jags) 
#library(modeest) 
library(gplots)
# library(rdrop2)
```


```{r Source input data and create stock composition dataframe, cache=TRUE, message=FALSE, echo=FALSE, warning=FALSE}

# stock compositions from multinomial bias correction model
raw_GSI <- read.csv("input files/input_bias_multinomial2.csv")

# input model for sampling bias correction
model.2 <- readRDS("input files/stock_year_sampbias.brm.fit.Feb212019")

# raw_GSI: change year to factor 
raw_GSI$yearF <- as.factor(raw_GSI$year)

dwrite <- data.frame( yearF = rep(unique(raw_GSI$yearF),1),
                      gear2 = rep("Test Fishery",30),
                      samp_bias = rep(0,30))

# produce CORRECTED sub-stock comps
stock_comps <- cbind(dwrite, predict(model.2, newdata = dwrite,  probs= c(0.025,0.5, 0.975))) %>%
  rename("Upper Lakes and Mainstem" = "P(Y = 30)") %>%
  rename("Teslin River" = "P(Y = 31)") %>%
  rename("Carmacks" = "P(Y = 32)") %>%
  rename("Middle Mainstem" = "P(Y = 33)") %>%
  rename("Pelly" = "P(Y = 34)") %>%
  rename("Stewart" = "P(Y = 35)") %>%
  rename("Lower Mainstem" = "P(Y = 36)") %>%
  rename("White-Donjek" = "P(Y = 38)")


# create in-filled stock comps for years missing data
# create each individual 'infilled' year data
y1984 <- c(1984,"Infill", 0,
          mean(c(stock_comps[2,4],stock_comps[3,4])),
          mean(c(stock_comps[2,5],stock_comps[3,5])),
          mean(c(stock_comps[2,6],stock_comps[3,6])),
          mean(c(stock_comps[2,7],stock_comps[3,7])),
          mean(c(stock_comps[2,8],stock_comps[3,8])),
          mean(c(stock_comps[2,9],stock_comps[3,9])),
          mean(c(stock_comps[2,10],stock_comps[3,10])),
          mean(c(stock_comps[2,11],stock_comps[3,11])))

y1988 <- c(1988,"Infill", 0,
          mean(c(stock_comps[5,4],stock_comps[6,4])),
          mean(c(stock_comps[5,5],stock_comps[6,5])),
          mean(c(stock_comps[5,6],stock_comps[6,6])),
          mean(c(stock_comps[5,7],stock_comps[6,7])),
          mean(c(stock_comps[5,8],stock_comps[6,8])),
          mean(c(stock_comps[5,9],stock_comps[6,9])),
          mean(c(stock_comps[5,10],stock_comps[6,10])),
          mean(c(stock_comps[5,11],stock_comps[6,11])))

y1989 <- c(1989,"Infill", 0,
          mean(c(stock_comps[5,4],stock_comps[6,4])),
          mean(c(stock_comps[5,5],stock_comps[6,5])),
          mean(c(stock_comps[5,6],stock_comps[6,6])),
          mean(c(stock_comps[5,7],stock_comps[6,7])),
          mean(c(stock_comps[5,8],stock_comps[6,8])),
          mean(c(stock_comps[5,9],stock_comps[6,9])),
          mean(c(stock_comps[5,10],stock_comps[6,10])),
          mean(c(stock_comps[5,11],stock_comps[6,11])))

y1990 <- c(1990,"Infill", 0,
          mean(c(stock_comps[5,4],stock_comps[6,4])),
          mean(c(stock_comps[5,5],stock_comps[6,5])),
          mean(c(stock_comps[5,6],stock_comps[6,6])),
          mean(c(stock_comps[5,7],stock_comps[6,7])),
          mean(c(stock_comps[5,8],stock_comps[6,8])),
          mean(c(stock_comps[5,9],stock_comps[6,9])),
          mean(c(stock_comps[5,10],stock_comps[6,10])),
          mean(c(stock_comps[5,11],stock_comps[6,11])))

y1998 <- c(1998,"Infill", 0,
          mean(c(stock_comps[12,4],stock_comps[13,4])),
          mean(c(stock_comps[12,5],stock_comps[13,5])),
          mean(c(stock_comps[12,6],stock_comps[13,6])),
          mean(c(stock_comps[12,7],stock_comps[13,7])),
          mean(c(stock_comps[12,8],stock_comps[13,8])),
          mean(c(stock_comps[12,9],stock_comps[13,9])),
          mean(c(stock_comps[12,10],stock_comps[13,10])),
          mean(c(stock_comps[12,11],stock_comps[13,11])))

# merge indivdiual infill years into single data_frame
infill <- as.data.frame(rbind(y1984, y1988, y1989, y1990, y1998)) %>%
  rename(yearF = V1) %>%
  rename(gear2 = V2) %>%
  rename(samp_bias = V3) %>%
  rename("Upper Lakes and Mainstem" = V4) %>%
  rename("Teslin River" = V5) %>%
  rename("Carmacks" = V6) %>%
  rename("Middle Mainstem" = V7) %>% 
  rename("Pelly" = V8) %>%
  rename("Stewart" = V9) %>%
  rename("Lower Mainstem" = V10) %>%
  rename("White-Donjek" = V11)

row.names(infill) <- NULL
infill$samp_bias <- as.numeric(levels(infill$samp_bias))[infill$samp_bias]

infill$`Upper Lakes and Mainstem` <- as.numeric(levels(infill$`Upper Lakes and Mainstem`))[infill$`Upper Lakes and Mainstem`]
infill$`Teslin River` <- as.numeric(levels(infill$`Teslin River`))[infill$`Teslin River`]
infill$Carmacks <- as.numeric(levels(infill$Carmacks))[infill$Carmacks]
infill$`Middle Mainstem` <- as.numeric(levels(infill$`Middle Mainstem`))[infill$`Middle Mainstem`]
infill$Pelly <- as.numeric(levels(infill$Pelly))[infill$Pelly]
infill$Stewart <- as.numeric(levels(infill$Stewart))[infill$Stewart]
infill$`Lower Mainstem` <- as.numeric(levels(infill$`Lower Mainstem`))[infill$`Lower Mainstem`]
infill$`White-Donjek` <- as.numeric(levels(infill$`White-Donjek`))[infill$`White-Donjek`]

# join infill and stock_comps dataframes for full dataframe of stock compositions for all years
SC <- full_join(infill, stock_comps) %>%
  arrange(yearF)


# put SC data frame in long format
SC_long <- SC %>%
  gather(key = substock, value = proportion, 4:11) %>%
  arrange(yearF)


```


```{r Source input data and create brood table dataframe, cache=TRUE, message=FALSE, echo=FALSE}
# Run Reconstruction brood table
RR <- read.delim("input files/RR_postsamp.txt")

# create brood table from RR input file, using median value from bayesian posterior distribution for each parameter
SPWN <- apply(RR[,244:277], 2, median)
ESC <- apply(RR[,37:70], 2, median)
AGE <- as.data.frame(apply(RR[,580:727], 2, median)) %>%
  mutate(yearF = as.factor(rep(c(1982:2018),c(4)))) %>%
  mutate(age_class = as.factor(rep(c(1,2,3,4),c(37,37,37,37)))) %>%
  rename(prob = "apply(RR[, 580:727], 2, median)") %>%
  filter(yearF != "2016") %>%
  filter(yearF != "2017") %>%
  filter(yearF != "2018") %>%
  spread(key = age_class, value = prob) %>%
  rename(age_4 = "1") %>%
  rename(age_5 = "2") %>%
  rename(age_6 = "3") %>%
  rename(age_7 = "4")

qq <- as.data.frame(cbind(SPWN, ESC)) %>%
  mutate(HARV = ESC - SPWN) %>%
  mutate(yearF = as.factor(c(1982:2015)))

qqq <- full_join(qq, AGE) %>%
  select(yearF, SPWN, ESC, HARV, age_4, age_5, age_6, age_7)

# full brood table including SPWN, ESC, AGE and all substock compositions
BROOD <- full_join(qqq, SC)

```



```{r Source input data and create border counts, cache=FALSE, message=FALSE, echo=FALSE}
# read in border counts (fish wheel and sonar)
border_counts <- read.delim("input files/input_bordercounts2.txt")

# modify border counts data frame

# sum border counts by year
# View(border_counts %>%
#       na.omit() %>%
#       group_by(year) %>%
#       summarise(sum = sum(count)))

# add column of counts per year to new dataframe bc2
bc2 <- border_counts %>% 
  mutate(sum = case_when(year == "1985" ~ 1321,
                         year == "1986" ~ 1998,
                         year == "1987" ~ 938,
                         year == "1988" ~ 976,
                         year == "1989" ~ 1065,
                         year == "1990" ~ 1361,
                         year == "1991" ~ 1726,
                         year == "1992" ~ 1889,
                         year == "1993" ~ 1241,
                         year == "1994" ~ 1290,
                         year == "1995" ~ 2215,
                         year == "1996" ~ 1749,
                         year == "1997" ~ 2221,
                         year == "1998" ~ 1080,
                         year == "1999" ~ 914,
                         year == "2000" ~ 1494,
                         year == "2001" ~ 3986,
                         year == "2002" ~ 1065,
                         year == "2003" ~ 1276,
                         year == "2004" ~ 1361,
                         year == "2005" ~ 81529,
                         year == "2006" ~ 73691,
                         year == "2007" ~ 41697,
                         year == "2008" ~ 38097,
                         year == "2009" ~ 69963,
                         year == "2010" ~ 35074,
                         year == "2011" ~ 51271,
                         year == "2012" ~ 34747,
                         year == "2013" ~ 30725,
                         year == "2014" ~ 63462,
                         year == "2015" ~ 84015,
                         year == "2016" ~ 72329))

# create proportion of total run count per julian day count
bc2$prop <- NA
bc2$prop <- bc2$count/bc2$sum
```

```{r source input data and create individual GSI dataframe, cache=TRUE, message=FALSE, echo=FALSE}
#read in Genetic Stock ID data from fish collected at border 
# ("probability" is the probability individual fish originated from stock X)
GSI <- read.csv("input files/input_Yukon_GSI_82_05_longform2.csv")

# modify individual GSI assignment dataframe
GSI.region <- subset(GSI, prob > 0.5)

# create data frame of number of samples per day and year 
GSI.counts <- plyr::ddply(GSI.region,c("year","julian_date"),function(x){
  count <- dim(x)[1]
  data.frame(count)
})

# add column of gsi counts per year to new dataframe: only keep samples with prob > 0.5
gsi2 <- GSI.counts %>% 
  dplyr::mutate(year_count = case_when(year == "1982" ~ 124,
                                year == "1983" ~ 141,
                                year == "1985" ~ 149,
                                year == "1986" ~ 149,
                                year == "1987" ~ 148,
                                year == "1991" ~ 147,
                                year == "1992" ~ 149,
                                year == "1993" ~ 149,
                                year == "1994" ~ 149,
                                year == "1995" ~ 149,
                                year == "1996" ~ 144,
                                year == "1997" ~ 150,
                                year == "1998" ~ 0,
                                year == "1999" ~ 147,
                                year == "2000" ~ 148,
                                year == "2001" ~ 149,
                                year == "2002" ~ 150,
                                year == "2003" ~ 148,
                                year == "2004" ~ 131,
                                year == "2005" ~ 142,
                                year == "2006" ~ 150,
                                year == "2007" ~ 149,
                                year == "2008" ~ 452,
                                year == "2009" ~ 646,
                                year == "2010" ~ 467,
                                year == "2011" ~ 497,
                                year == "2012" ~ 344,
                                year == "2013" ~ 290,
                                year == "2014" ~ 708,
                                year == "2015" ~ 1026,
                                year == "2016" ~ 728))

# create proportion of total run count per julian day count
gsi2$prop <- NA
gsi2$prop <- gsi2$count/gsi2$year_count

```


```{r create dataframes for State Space models, cache=TRUE, echo=FALSE}

# escapement data
esc_data <- BROOD %>%
  select(yearF, ESC, 11:18) %>%
  gather(key = region, value = proportion, 3:10) %>%
  mutate(esc_stock = ESC*proportion) %>%
  mutate(reg_code = case_when(region == "Carmacks" ~ "1",
                              region == "Lower Mainstem" ~ "2",
                              region == "Middle Mainstem" ~ "3",
                              region == "Pelly" ~ "4",
                              region == "Stewart" ~ "5",
                              region == "Teslin River" ~ "6",
                              region == "Upper Lakes and Mainstem" ~ "7",
                              region == "White-Donjek" ~ "8")) %>%
  mutate(truecount = case_when(yearF == 1984 ~ 0,
                              yearF == 1988 ~ 0,
                              yearF == 1989 ~ 0,
                              yearF == 1990 ~ 0,
                              yearF == 1998 ~ 0,
                              yearF < 1984 ~ 1,
                              yearF >= 1985 & yearF <= 1987 ~ 1,
                              yearF >= 1991 & yearF <= 1997 ~1,
                              yearF >= 1999 & yearF <= 2005 ~1,
                              yearF >= 2006 ~1)) %>%
  select(yearF, region, esc_stock, reg_code, truecount)

# harvest data
harv_data <- BROOD %>%
  select(yearF, HARV, 11:18) %>%
  gather(key = region, value = proportion, 3:10) %>%
  mutate(harv_stock = HARV*proportion) %>%
  mutate(reg_code = case_when(region == "Carmacks" ~ "1",
                              region == "Lower Mainstem" ~ "2",
                              region == "Middle Mainstem" ~ "3",
                              region == "Pelly" ~ "4",
                              region == "Stewart" ~ "5",
                              region == "Teslin River" ~ "6",
                              region == "Upper Lakes and Mainstem" ~ "7",
                              region == "White-Donjek" ~ "8")) %>%
  mutate(truecount = case_when(yearF == 1984 ~ 0,
                              yearF == 1988 ~ 0,
                              yearF == 1989 ~ 0,
                              yearF == 1990 ~ 0,
                              yearF == 1998 ~ 0,
                              yearF < 1984 ~ 1,
                              yearF >= 1985 & yearF <= 1987 ~ 1,
                              yearF >= 1991 & yearF <= 1997 ~1,
                              yearF >= 1999 & yearF <= 2005 ~1,
                              yearF >= 2006 ~1)) %>%
  select(yearF, region, harv_stock, reg_code, truecount)

# age data
age_data <- BROOD %>%
  select(yearF, age_4, age_5, age_6, age_7, 11:18) %>%
  gather(key = region, value = proportion, 6:13) %>%
  mutate(reg_code = case_when(region == "Carmacks" ~ "1",
                              region == "Lower Mainstem" ~ "2",
                              region == "Middle Mainstem" ~ "3",
                              region == "Pelly" ~ "4",
                              region == "Stewart" ~ "5",
                              region == "Teslin River" ~ "6",
                              region == "Upper Lakes and Mainstem" ~ "7",
                              region == "White-Donjek" ~ "8")) %>%
  mutate(gooddata = case_when(yearF == 1984 ~ 0,
                              yearF == 1988 ~ 0,
                              yearF == 1989 ~ 0,
                              yearF == 1990 ~ 0,
                              yearF == 1998 ~ 0,
                              yearF < 1984 ~ 1,
                              yearF >= 1985 & yearF <= 1987 ~ 1,
                              yearF >= 1991 & yearF <= 1997 ~1,
                              yearF >= 1999 & yearF <= 2005 ~1,
                              yearF >= 2006 ~1)) %>%
  select(yearF, region, age_4, age_5, age_6, age_7, reg_code, gooddata)




# obtain aggrgate exploitation rates for state space models
exploitation_rate <- read.csv("input files/input_yukon_chin_bt_red.csv")

er <- exploitation_rate %>%
  select(year, er)

```


This memo provides a brief summary of progress to date on Objectives 1-4 of the AYK-SSI grant “Yukon Chinook harvest-population diversity tradeoffs”. The overarching goal of the project is to describe CDN-origin Yukon Chinook population diversity, understand the extent to which this diversity leads to tradeoffs with harvest, and develop a simulation model that allows us to begin to explore how well alternative management procedures meet fishery and population diversity objectives.

The specific objectives of the project are to: 
1.	Extract genetic material from archived scale samples to determine sub-stock composition of CDN-origin Yukon Chinook returns from 1982 to present.
2.	Use estimates of sub-stock composition (from Obj. 1) together with an existing CDN-Yukon River Basin wide run-reconstruction to reconstruct harvest, spawner abundance and age-composition at a sub-stock level.
3.	Fit age-structured state-space stock-recruitment models to the sub-stock reconstructions (Obj. 2) to characterize Chinook population diversity within the CDN portion of the Yukon River Basin.  
4.	Quantify equilibrium tradeoffs between harvest and conservation of population diversity across a range of mixed-stock harvest rates.
5.	Develop closed loop simulations parameterized by the data from Obj. 3 to quantify the predicted fishery and population diversity consequences of a range of alternative management procedures that are comprised on Alaskan harvest rates, border passage goals and within-Canada harvest strategies. 

**Objective 1**
On average, 1300 Chinook scale samples have been collected annually from fish wheels (1982–2008) and gillnets (2005–present) at a range of locations on the Yukon River near the U.S. - Canada border (Figure 1). These samples have typically been taken over the duration of the annual upstream adult migration, with the number of samples taken each day roughly proportional to run size. Since 2006 tissue samples have also been collected for genetic stock ID by assigning each fish back to one of the eight major sub-stock groupings using microsatellite markers (between 293 and 1026 fish per year) (Beacham et al. 2006) (Figure 1). We extended this time series of sub-stock composition estimates by sub-sampling archived scale samples from 1982 to 2005 (between 124 and 134 fish per year; between 109 and 835 fish per year for 2006-2016; Figure 2).

In most years, the scales we sub-sampled for genetic stock ID appear to be representative of the run (compare distribution of grey vs. red bars in Figure 2). However, there were clearly some years (e.g., 2001) where we are missing a substantial portion of the run.  When the new (1982-2005) and existing (2008-2016) estimates of sub-stock composition are combined they indicate that there are (no surprise) substantial differences among sub-stocks in their contribution to total returns in a given year (e.g., Teslin is largest contributor and the Upper mainstem is the smallest) and that there is considerable inter-annual variation within sub-stocks in their contributions to total returns (Figure 3). 


```{r Run timing distriubtion with scale sampling distribution for Figure 2, echo=FALSE}
# create sample size number dataframe for geom_text in plot
dat_text <- data.frame(label = c("124", "141", "149","149", "148", "147",
                                 "149", "149", "149","149", "144", "150", "147", "148",
                                 "149", "150", "148","131", "142","150","149","452","646",
                                 "467","497","344","290","708","1026","728"),
                       year = c(1982,1983,1985,1986,1987,1991,
                                1992,1993,1994,1995,1996,1997, 1999,2000,
                                2001,2002,2003,2004,2005,2006, 2007,2008,2009,
                                2010,2011,2012,2013,2014,2015,2016))

# Run size distribution and GSI size and distribution, memo Fig 2
n <- c(1982:1987,1991:1997,1999:2016)
bc3 <- bc2 %>%
  filter(year %in% n)

gsi3 <- gsi2 %>%
  filter(year %in% n)
```

```{r Plot Figure 2: border counts and gsi sample size, echo=FALSE, warning=FALSE, fig.align='center', fig.dim=c(5,5), fig.caption="Figure 2. **Run distribution and scale sampling distribution.** Daily estimates of Chinook entering the Yukon (light grey bars) and distribution of scales sampled from run to determine sub-stock ID. Overall, the scale sub-sampling has been generally representative of the observed run. However, there are a some clear mismatches between the observed run our scale sub-sampling (e.g., 2001 where the scales sub-sampled for GSI did not overlap with much of the run). The total number of scales analyzed each year to date is in the upper left corner of each panel. Note that scales were unavailable for 1988-1990, and 1998 and we do not currently have information on the distribution of the run in 1982 and 1983. We also do not have julian dates for the GSI samples in 2010 and 2012. Given the high sample size for those years, we assume the run was adequately represented. For reference in a non leap year July 19th is equivalent to Julian day 200."}

# plot figure border counts and gsi sample size 
ggplot(bc3, aes(x=julian, y = prop)) +
  geom_bar(stat = "identity") +
  geom_bar(data = gsi3 %>% filter(julian_date < 1000), aes(x=julian_date, y=prop), 
           fill="red", alpha=0.5, stat = "identity") +
  scale_x_continuous(limits=c(150,300), breaks = c(150,175,200,225,250,275,300)) +
  xlab("Julian day") +
  ylab("Run size and GSI sample size") +
  facet_wrap(~year, scales = "free_y") +
  theme_bw() +
  theme(axis.text.x = element_text(size=9, angle = 45, hjust = 1)) +
  theme(strip.text = element_text(size=10)) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  geom_text(data = dat_text, mapping = aes(x = -Inf, y = -Inf, label = label),
            hjust = -0.3, vjust = -3, size=2.5)
```


```{r Figure 3: Sub-stock proportions over time ~facet_wrap(substock), echo=FALSE, fig.align='center', fig.dim=c(5,5), fig.caption="Figure 3. **Sub-stock total ru proportions.** Proportion (+/- SD) of each sub-stock in the test fishery catch from 1982-2016. The dotted line at 2008 marks the transition from fish wheels to gillnets and sonar used to estimate abundance. The years 1984, 1989-1991, 1998, and 2006-2007 are currently missing data."} 

ggplot(SC_long, aes(x = yearF, y = proportion*100)) + 
  geom_bar(stat = "identity", colour = "black", fill="light grey") +
  #geom_errorbar(aes(ymin=lower, ymax=upper), width=.2) +
  geom_vline(xintercept = 2008, lty = "dotted") +
  xlab("Year") +
  ylab("Proportion of test fishery catch (%)") +
  scale_x_discrete(breaks = c("1984","1988","1992","1996","2000","2004","2008","2012")) +
  theme_bw() +
  facet_wrap(~substock, ncol=3, scales = "free_y") +
  theme(strip.text = element_text(size=10)) +
  theme(axis.text.x = element_text(angle=90, hjust=1)) +
  theme(axis.text = element_text(size=10)) +
  theme(axis.title = element_text(size=10)) +
  theme(legend.justification = "center")
```



The sub-stock composition estimates for small stocks are based on very small sample sizes (e.g., less than 5 fish in a given year for the Upper Lakes and Mainstem sub-stock!) and so should be interpreted with an abundance of caution. Additionally, it is currently unknown if samples collected by fish wheel are biased towards specific sub-stocks (e.g., due to bank orientation). In order to quantify this we are conducting molecular analyses on 7 more years of scale samples (2006-2013) that can be compared to stock-composition estimates derived from the gill-net test fishery that has been in operation (alongside the fish wheels) since 2006.

**Objective 2**
The Yukon River Panel Joint Technical Committee (JTC) uses multiple sources of information to reconstruct total returns of Chinook to the CDN portion of the Yukon along with harvest and age-composition (JTC 2017) (Figure 4). These sources of information include: (1) border passage estimates from radiotelemetry data (2002-2004) and mainstem sonar projects (2005-2007), (2) aerial spawner surveys from multiple systems as indices of relative abundance (1981-2001), (3) estimates of harvest in both US and CDN fisheries, and (4) age composition data from fish wheel and gillnet surveys near the US-Canada border as well as from US harvest. The resulting brood table forms the basis of the current data used to fit a basin wide stock-recruitment model (JTC 2017). 

In order to develop first pass run reconstructions at the sub-stock level we simply applied the estimated sub-stock composition estimates from Objective 1 to the basin wide harvest and escapement estimates from the JTC. We infilled years with missing composition data by using the average sub-stock proportions from the two closest years with empirical estimates. For example, sub-stock proportions for 1984 were the average of the 1983 and 1985 sub-stock proportions. Our sub-stock reconstructions ignore uncertainty in the sub-stock contributions (due to both the precision of the GSI and small sample sizes), and assume that all sub-stocks experienced similar exploitation rates. A logical next step is to propagate these two sources of uncertainty in sub-stock contributions and work with current and former DFO staff in Whitehorse to develop a more nuanced understanding of historic sub-stock vulnerability to the fisheries that occurred in the Yukon Territory. 

The resulting sub-stock run-reconstructions (not surprisingly) tended to mirror the patterns in the aggregate CDN-origin reconstruction (Figure 4) but are scaled to the size of the individual sub-stocks. The first half of the time series is characterized by high harvest rates, which have declined dramatically in recent years coincident with substantial increases in escapement for some sub-stocks (e.g., Teslin and Middle Mainstem) but not others (e.g., Lower Mainstem and White-Donjek) (Figure 5). 



```{r Functions for state-space stock-recruitment models, cache=TRUE, echo=FALSE}
# Posterior summary function
post.summ = function(post.samp, var) {
  post.samp = as.matrix(post.samp)
  
  # if parameter is indexed
  if(substr(var, nchar(var), nchar(var)) == "[") {
    post = post.samp[,substr(colnames(post.samp), 1, nchar(var)) == var]
    summ = apply(post, 2, function(x) c(mean = mean(x), sd = sd(x), quantile(x, c(0.5, 0.025, 0.975))))
    return(summ)
  }
  
  # if parameter is not indexed
  if(substr(var, nchar(var), nchar(var)) != "[") {
    post = post.samp[,substr(colnames(post.samp), 1, nchar(var)) == var]
    summ = c(mean = mean(post), sd = sd(post), quantile(post, c(0.5, 0.025, 0.975)))
    return(summ)
  }
}
```



```{r Bayes model, echo=FALSE}
modelFilename = "dep_mod.txt"
cat("
    model {
    # priors for SR portion
    lnalpha ~ dunif(0, 3) 
    #beta ~ dunif(0,10)
    beta ~ dnorm(bpmu,bptau)
    beta.prior ~ dnorm(bpmu,bptau) #for plotting and checking
    tau.R ~ dgamma(0.01,0.01)  # white noise process error      
    phi ~ dunif(-0.99, 0.99)   # autocorrelation coefficient                                              
    log.resid.0 ~ dnorm(0, tau.red)  # starting residual for AR1 process
    
    # Ricker SR with AR1 process on log recruitment residuals for years with brood year spawners
    for (y in (A+a.min):nRyrs) {
    log.R[y] ~ dnorm(log.R.mean.2[y], tau.R)  # true state R is lognormally distributed around the prediction given by SR with AR1
    R[y] <- exp(log.R[y])
    log.R.mean.1[y] <- lnalpha + log(S[y-a.max]) - beta * S[y-a.max]
    log.resid.a[y] <- log.R[y] - log.R.mean.1[y]
    }             
    
    log.R.mean.2[A+a.min] <- log.R.mean.1[A+a.min] + phi * log.resid.0
    
    for (y in (A+a.min+1):nRyrs) {
    log.R.mean.2[y] <- log.R.mean.1[y] + phi * log.resid.a[y-1]
    }
    
    #derived quantities
    tau.red <- tau.R * (1 - phi * phi)
    sigma.red <- 1 / sqrt(tau.red)
    sigma.R <- 1 / sqrt(tau.R)
    alpha <- exp(lnalpha)
    log.resid <- log.resid.a[(A+a.min):nRyrs]
    
    # First `a.max` years of recruits, for which there is no spawner link
    mean.log.R0 ~ dnorm(0, 1E-4) 
    mean.R0 <- exp(mean.log.R0)
    tau.R0 ~ dgamma(0.1,0.1)
    sigma.R0 <- 1/sqrt(tau.R0)
    for (y in 1:a.max) {
    log.R[y] ~ dnorm(mean.log.R0, tau.R0)   
    R[y] <- exp(log.R[y])
    }
    
    # biological reference points: derived quantities
    lnalpha.c <- lnalpha + (sigma.R * sigma.R)/2/(1-phi * phi)
    S.max <- 1/beta
    S.eq <- lnalpha.c * S.max
    S.msy <- S.eq * (0.5 - 0.07 * lnalpha.c)
    U.msy <- lnalpha.c * (0.5 - 0.07 * lnalpha.c)
    
    # Maturity schedule: here we use a common maturation schedule to draw the brood year specific schedules;
    prob[1] ~ dbeta(1,1)
    prob[2] ~ dbeta(1,1)
    prob[3] ~ dbeta(1,1)
    pi[1]<- prob[1]
    pi[2] <- prob[2] * (1 - pi[1])
    pi[3] <- prob[3] * (1 - pi[1] - pi[2])
    pi[4] <- 1 - pi[1] - pi[2] - pi[3]
    
    D.scale ~ dunif(.045,1)
    D.sum <- 1 / (D.scale * D.scale)
    for (a in 1:A) {
    gamma[a] <- D.sum * pi[a]
    for (y in 1:(Y+A-1)) {                                                    
    g[y,a] ~ dgamma(gamma[a],1.0)
    p[y,a] <- g[y,a]/sum(g[y,])
    }
    }
    
    # Calculate the numbers at age matrix as brood year recruits at age*proportion that matured that year
    for (t in 1:Y) {
    for(a in 1:A){
    N.ta[t,a] <- R[t+A-a] * p[t+A-a,a]
    }
    }
    
    ## OBSERVATION SUBMODEL ##
    # multinomial scale sampling
    for (t in 1:Y) {
    for (a in 1:A) {
    q[t,a] <- N.ta[t,a]/N[t]
    }
    x[t,1:A] ~ dmulti(q[t,1:A], n[t])
    }
    
    for (t in 1:Y) {
    # get observation tau's from assumed CV's
    log.sigma.C[t] <- sqrt(log((C.cv[t]^2) + 1))
    log.tau.C[t] <- 1/log.sigma.C[t]^2
    log.sigma.S[t] <- sqrt(log((S.cv[t]^2) + 1))
    log.tau.S[t] <- 1/log.sigma.S[t]^2
    
    # catch model
    U[t] ~ dunif(0.01, 0.99)
    N[t] <- sum(N.ta[t,1:A])
    S[t] <- N[t] * (1 - U[t])
    
    C[t] <- N[t] * U[t]
    log.C[t] <- log(C[t])
    C.obs[t] ~ dlnorm(log.C[t], log.tau.C[t])
    
    # escapement model
    log.S[t] <- log(S[t])
    S.obs[t] ~ dlnorm(log.S[t], log.tau.S[t])
    }
    
    }
    
    ", fill=TRUE, file=modelFilename)

#file.show(modelFilename)
```



```{r Load data for Bayes stock recruit model, echo=FALSE}
# Load data
age <- age_data %>%
  filter(yearF < 2016) %>%
  rename(year = yearF) %>%
  rename(truecount = gooddata)

esc <- esc_data %>%
  filter(yearF < 2016) %>%
  rename(year = yearF)

harv <- harv_data %>%
  filter(yearF < 2016) %>%
  rename(year = yearF)
```


```{r import posterior files, so don't have to re-run each SR model, echo=FALSE}
require(tidyverse)

post.ag <- read_csv(("EN2438_Rmark_posteriors/aggregate_posteriors_2019-04-08.csv"))

post.YC <- read_csv(("EN2438_Rmark_posteriors/carmacks_posteriors_2019-04-08.csv"))

post.YlC <- read_csv(("EN2438_Rmark_posteriors/lower.mainstem_posteriors_2019-04-08.csv"))

post.Ym <- read_csv(("EN2438_Rmark_posteriors/midmainstem_posteriors_2019-04-08.csv"))

post.YP <- read_csv(("EN2438_Rmark_posteriors/pelly_posteriors_2019-04-08.csv"))

post.YS <- read_csv(("EN2438_Rmark_posteriors/stewart_posteriors_2019-04-08.csv"))

post.YT <- read_csv(("EN2438_Rmark_posteriors/teslin_posteriors_2019-04-08.csv"))

post.Yu <- read_csv(("EN2438_Rmark_posteriors/upper_posteriors_2019-04-08.csv"))

post.YWD <- read_csv(("EN2438_Rmark_posteriors/whitedonjek_posteriors_2019-04-08.csv"))
```



```{r Aggregate Data and model run, echo=FALSE}
# Create Aggregate data frames for data: age, esc, harv
# create input variables for Bayesian model run

age.ag <- age %>%
  distinct(year, age_4, age_5, age_6, age_7, truecount) %>%
  mutate(region = "aggregate") %>%
  mutate(reg_code = "0") %>%
  dplyr::select(year, region, age_4, age_5, age_6, age_7, reg_code, truecount) %>%
  as.data.frame()

esc.ag <- esc %>%
  group_by(year) %>%
  mutate(spawn = sum(esc_stock)/1000) %>%
  ungroup() %>%
  mutate(region = "aggregate") %>%
  mutate(reg_code = "0") %>%
  dplyr::select(year, region, reg_code, truecount, spawn) %>%
  distinct(year, region, spawn, reg_code,truecount) %>%
  as.data.frame(esc.ag)

harv.ag <- harv %>%
  group_by(year) %>%
  mutate(harvest = sum(harv_stock)/1000) %>%
  ungroup() %>%
  mutate(region = "aggregate") %>%
  mutate(reg_code = "0") %>%
  dplyr::select(year, region, reg_code, truecount, harvest) %>%
  distinct(year, region, harvest, reg_code, truecount) %>%
  as.data.frame(harv.ag)

# mean escapement over time series
SMAX <- mean(esc.ag$spawn) 

# turn SMAX into beta prior
bpmu = 1/SMAX

# set CV on prior
bptau = 1/((1)^2)

Y = nrow(age.ag)          # number of calendar years observed
a.min = 4                 # minimum age class in data set
a.max = 7                 # maximum age class in data set
A = a.max - a.min + 1     # number of age classes
nRyrs = Y + A - 1         # number of recruitment years (see model code for details)
years = age.ag[,"year"]

# escapement: assume a 30% observation CV if directly observed, 50% otherwise
S.cv = ifelse(esc.ag[,4] == 1, 0.3, 0.5)
S.obs = esc.ag[,5]

# harvest: assume a 15% observation CV if directly observed, 30% otherwise
C.cv = ifelse(harv.ag[,4] == 1, 0.15, 0.30)
C.obs = harv.ag[,5]

# age composition: assume a ESS of 100 if directly observed, 25 otherwise
ESS = ifelse(age.ag$truecount == 1, 100, 25) 
X <- age.ag %>%
  select(3:6)
X <- as.matrix(X)
X = t(apply(X, 1, function(x) x/sum(x)))
X = round(apply(X, 2, function(x) x * ESS))
colnames(X) = NULL
n = rowSums(X)        # n is slightly different than ESS because of rounding errors
x=X

# r Jags inputs for Aggregate group
jags.data = list('Y','a.min','a.max','A','nRyrs','S.cv','S.obs','C.cv','C.obs',
                 'x','n','bpmu','bptau')

jags.parms = c("R", "N", "S", "U", "alpha", "beta", "lnalpha", "phi", "C", "log.resid",
               "log.resid.0","sigma.R", "lnalpha.c", "mean.log.R0", "pi", "q", 
               "mean.R0", "sigma.R0","S.msy", "S.max", "S.eq", "U.msy", "gamma", 
               "D.sum", "p","log.S","beta.prior")
```


```{r Fit Model for Aggregate group, echo=FALSE, results="hide", eval=FALSE}
ptm = proc.time()

jagsfit.ag <- jags.parallel(data=jags.data,  parameters.to.save=jags.parms,n.thin=5,
                            n.iter=300000, model.file=modelFilename,n.burnin = 50000,n.chains=3)

endtime = proc.time()-ptm
# endtime[3]/60
```


```{r Aggregate posterior editing, echo=FALSE}
# post.ag = as.mcmc(jagsfit.ag)
# rm(jagsfit.ag)

mypost.ag = as.matrix(post.ag, chain=F)
rm(post.ag)


# export posterior file
# sys_time <- format(Sys.time(),"%Y-%m-%d")

# paste("aggregate_posteriors_",sys_time,".csv")

# write.csv(mypost.ag, paste("aggregate_posteriors_",sys_time,".csv", sep=""), row.names = FALSE)

# drop_upload( , path = , mode = "add", mute = FALSE)



R = post.summ(mypost.ag,"R[")
S = post.summ(mypost.ag, "S[")
N = post.summ(mypost.ag, "N[")
U = post.summ(mypost.ag, "U[")
resid = post.summ(mypost.ag, "log.resid[")
PP = post.summ(mypost.ag, "pi[")
alpha = post.summ(mypost.ag, "alpha")
beta = post.summ(mypost.ag, "beta")
phi = post.summ(mypost.ag, "phi")
sigma = post.summ(mypost.ag, "sigma.R")
S.msy = post.summ(mypost.ag, "S.msy")
S.eq = post.summ(mypost.ag, "S.eq")
S.max = post.summ(mypost.ag, "S.max")
U.msy = post.summ(mypost.ag, "U.msy")


output_summary_ag <- cbind(R, S, N, U, resid, PP, alpha, beta, phi, sigma, S.msy, S.max, U.msy) %>%
  as.data.frame()
output_summary_ag$region <- rep(c("aggregate"), (5))



# subsample alpha and beta
alphabeta_ag <- mypost.ag %>%
  as.data.frame() %>%
  select(alpha, beta) %>%
  sample_n(100, replace=TRUE) %>%
  mutate(region = "aggregate")

# keep U.msy vector
U.msy.ag <- mypost.ag[,512] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "aggregate") %>%
  as.data.frame()

# keep alpha vector
alpha.ag <- mypost.ag[,1] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "aggregate") %>%
  as.data.frame()



rm(mypost.ag)

# gelman.diag(post.ag, multivariate = F)
```



```{r Carmacks data and model run, echo=FALSE}
# Create Carmacks data frames for data: age, esc, harv
# create input variables for Bayesian model run

age.YC <- age %>%
  filter(region == "Carmacks")
esc.YC <- esc %>%
  filter(region == "Carmacks") %>%
  mutate(spawn = esc_stock/1000)
harv.YC <- harv %>%
  filter(region == "Carmacks") %>%
  mutate(harvest = harv_stock/1000)


# mean escapement over time series
SMAX <- mean(esc.YC$spawn) 

# turn SMAX into beta prior
bpmu = 1/SMAX

# set CV on prior
bptau = 1/((1)^2)


Y = nrow(age.YC)          # number of calendar years observed
a.min = 4                 # minimum age class in data set
a.max = 7                 # maximum age class in data set
A = a.max - a.min + 1     # number of age classes
nRyrs = Y + A - 1         # number of recruitment years (see model code for details)
years = age.YC[,"year"]

# escapement: assume a 30% observation CV if directly observed, 50% otherwise
S.cv = ifelse(esc.YC[,5] == 1, 0.3, 0.5)
S.obs = esc.YC[,6]

# harvest: assume a 15% observation CV if directly observed, 30% otherwise
C.cv = ifelse(harv.YC[,5] == 1, 0.15, 0.30)
C.obs = harv.YC[,6]

# age composition: assume a ESS of 100 if directly observed, 25 otherwise
ESS = ifelse(age.YC$truecount == 1, 100, 25) 
X <- age.YC %>%
  select(3:6)
X <- as.matrix(X)
X = t(apply(X, 1, function(x) x/sum(x)))
X = round(apply(X, 2, function(x) x * ESS))
colnames(X) = NULL
n = rowSums(X)        # n is slightly different than ESS because of rounding errors
x=X



# r Jags inputs for Carmacks group
jags.data = list('Y','a.min','a.max','A','nRyrs','S.cv','S.obs','C.cv','C.obs',
                 'x','n','bpmu','bptau')

jags.parms = c("R", "N", "S", "U", "alpha", "beta", "lnalpha", "phi", "C", "log.resid",
               "log.resid.0","sigma.R", "lnalpha.c", "mean.log.R0", "pi", "q", 
               "mean.R0", "sigma.R0","S.msy", "S.max", "S.eq", "U.msy", "gamma", 
               "D.sum", "p","log.S","beta.prior")
```


```{r Fit Model for Carmacks group, echo=FALSE, results="hide", eval=FALSE}
ptm = proc.time()

jagsfit.YC <- jags.parallel(data=jags.data,  parameters.to.save=jags.parms,n.thin=5,
                            n.iter=300000, model.file=modelFilename,n.burnin = 50000,n.chains=3)

endtime = proc.time()-ptm
endtime[3]/60
```


```{r Carmacks posterior editing}
# post.YC = as.mcmc(jagsfit.YC)
# rm(jagsfit.YC)

mypost.YC = as.matrix(post.YC, chain=F)
rm(post.YC)


# write.csv(mypost.YC, paste("carmacks_posteriors_",sys_time,".csv", sep=""), row.names = FALSE)



R = post.summ(mypost.YC,"R[")
S = post.summ(mypost.YC, "S[")
N = post.summ(mypost.YC, "N[")
U = post.summ(mypost.YC, "U[")
resid = post.summ(mypost.YC, "log.resid[")
PP = post.summ(mypost.YC, "pi[")
alpha = post.summ(mypost.YC, "alpha")
beta = post.summ(mypost.YC, "beta")
phi = post.summ(mypost.YC, "phi")
sigma = post.summ(mypost.YC, "sigma.R")
S.msy = post.summ(mypost.YC, "S.msy")
S.eq = post.summ(mypost.YC, "S.eq")
S.max = post.summ(mypost.YC, "S.max")
U.msy = post.summ(mypost.YC, "U.msy")


output_summary_YC <- cbind(R, S, N, U, resid, PP, alpha, beta, phi, sigma, S.msy, S.max, U.msy) %>%
  as.data.frame()
output_summary_YC$region <- rep(c("Carmacks"), (5))

# subsample alpha and beta
alphabeta_YC <- mypost.YC %>%
  as.data.frame() %>%
  select(alpha, beta) %>%
  sample_n(100, replace=TRUE) %>%
  mutate(region = "Carmacks")


# keep U.msy vector
U.msy.YC <- mypost.YC[,512] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Carmacks") %>%
  as.data.frame()

# keep alpha vector
alpha.YC <- mypost.YC[,1] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Carmacks") %>%
  as.data.frame()

rm(mypost.YC)

```


```{r Lower Mainstem data and model run, echo=FALSE}
# Create lower mainstem data frames for data: age, esc, harv
# create input variables for Bayesian model run

age.YlC <- age %>%
  filter(region == "Lower Mainstem")
esc.YlC <- esc %>%
  filter(region == "Lower Mainstem") %>%
  mutate(spawn = esc_stock/1000)
harv.YlC <- harv %>%
  filter(region == "Lower Mainstem") %>%
  mutate(harvest = harv_stock/1000)

# mean escapement over time series
SMAX <- mean(esc.YlC$spawn) 

# turn SMAX into beta prior
bpmu = 1/SMAX

# set CV on prior
bptau = 1/((1)^2)

Y = nrow(age.YlC)         # number of calendar years observed
a.min = 4                 # minimum age class in data set
a.max = 7                 # maximum age class in data set
A = a.max - a.min + 1     # number of age classes
nRyrs = Y + A - 1         # number of recruitment years (see model code for details)
years = age.YlC[,"year"]

# escapement: assume a 30% observation CV if directly observed, 50% otherwise
S.cv = ifelse(esc.YlC[,5] == 1, 0.3, 0.5)
S.obs = esc.YlC[,6]

# harvest: assume a 15% observation CV if directly observed, 30% otherwise
C.cv = ifelse(harv.YlC[,5] == 1, 0.15, 0.30)
C.obs = harv.YlC[,6]

# age composition: assume a ESS of 100 if directly observed, 25 otherwise
ESS = ifelse(age.YlC$truecount == 1, 100, 25) 
X <- age.YlC %>%
  select(3:6)
X <- as.matrix(X)
X = t(apply(X, 1, function(x) x/sum(x)))
X = round(apply(X, 2, function(x) x * ESS))
colnames(X) = NULL
n = rowSums(X)        # n is slightly different than ESS because of rounding errors
x=X

# r Jags inputs for Lower Mainstem group
jags.data = list('Y','a.min','a.max','A','nRyrs','S.cv','S.obs','C.cv','C.obs',
                 'x','n','bpmu','bptau')

jags.parms = c("R", "N", "S", "U", "alpha", "beta", "lnalpha", "phi", "C", "log.resid",
               "log.resid.0","sigma.R", "lnalpha.c", "mean.log.R0", "pi", "q", 
               "mean.R0", "sigma.R0","S.msy", "S.max", "S.eq", "U.msy", "gamma", 
               "D.sum", "p","log.S","beta.prior")
```


```{r Fit Model for Lower Mainstem group, echo=FALSE, eval=FALSE}
ptm = proc.time()

jagsfit.YlC <- jags.parallel(data=jags.data,  parameters.to.save=jags.parms,n.thin=5,
                            n.iter=300000, model.file=modelFilename,n.burnin = 50000,n.chains=3)

endtime = proc.time()-ptm
#endtime[3]/60
```



```{r lower mainstem posterior editing, echo=FALSE}

# post.YlC = as.mcmc(jagsfit.YlC)
# rm(jagsfit.YlC)

mypost.YlC = as.matrix(post.YlC, chain=F)
rm(post.YlC)


# write.csv(mypost.YlC, paste("lower.mainstem_posteriors_",sys_time,".csv", sep=""), row.names = FALSE)


R = post.summ(mypost.YlC,"R[")
S = post.summ(mypost.YlC, "S[")
N = post.summ(mypost.YlC, "N[")
U = post.summ(mypost.YlC, "U[")
resid = post.summ(mypost.YlC, "log.resid[")
PP = post.summ(mypost.YlC, "pi[")
alpha = post.summ(mypost.YlC, "alpha")
beta = post.summ(mypost.YlC, "beta")
phi = post.summ(mypost.YlC, "phi")
sigma = post.summ(mypost.YlC, "sigma.R")
S.msy = post.summ(mypost.YlC, "S.msy")
S.eq = post.summ(mypost.YlC, "S.eq")
S.max = post.summ(mypost.YlC, "S.max")
U.msy = post.summ(mypost.YlC, "U.msy")


output_summary_YlC <- cbind(R, S, N, U, resid, PP, alpha, beta, phi, sigma, S.msy, S.max, U.msy) %>%
  as.data.frame()
output_summary_YlC$region <- rep(c("Lower Mainstem"), (5))

# subsample alpha and beta
alphabeta_YlC <- mypost.YlC %>%
  as.data.frame() %>%
  select(alpha, beta) %>%
  sample_n(100, replace=TRUE) %>%
  mutate(region = "Lower Mainstem")

# keep U.msy vector
U.msy.YlC <- mypost.YlC[,512] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Lower Mainstem") %>%
  as.data.frame()

# keep alpha vector
alpha.YlC <- mypost.YlC[,1] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Lower Mainstem") %>%
  as.data.frame()

rm(mypost.YlC)

```




```{r Middle Mainstem data and model run, echo=FALSE}
# Create Middle mainstem data frames for data: age, esc, harv
# create input variables for Bayesian model run

age.Ym <- age %>%
  filter(region == "Middle Mainstem")
esc.Ym <- esc %>%
  filter(region == "Middle Mainstem") %>%
  mutate(spawn = esc_stock/1000)
harv.Ym <- harv %>%
  filter(region == "Middle Mainstem") %>%
  mutate(harvest = harv_stock/1000)


# mean escapement over time series
SMAX <- mean(esc.Ym$spawn) 

# turn SMAX into beta prior
bpmu = 1/SMAX

# set CV on prior
bptau = 1/((1)^2)

Y = nrow(age.Ym)         # number of calendar years observed
a.min = 4                 # minimum age class in data set
a.max = 7                 # maximum age class in data set
A = a.max - a.min + 1     # number of age classes
nRyrs = Y + A - 1         # number of recruitment years (see model code for details)
years = age.Ym[,"year"]

# escapement: assume a 30% observation CV if directly observed, 50% otherwise
S.cv = ifelse(esc.Ym[,5] == 1, 0.3, 0.5)
S.obs = esc.Ym[,6]

# harvest: assume a 15% observation CV if directly observed, 30% otherwise
C.cv = ifelse(harv.Ym[,5] == 1, 0.15, 0.30)
C.obs = harv.Ym[,6]

# age composition: assume a ESS of 100 if directly observed, 25 otherwise
ESS = ifelse(age.Ym$truecount == 1, 100, 25) 
X <- age.Ym %>%
  select(3:6)
X <- as.matrix(X)
X = t(apply(X, 1, function(x) x/sum(x)))
X = round(apply(X, 2, function(x) x * ESS))
colnames(X) = NULL
n = rowSums(X)        # n is slightly different than ESS because of rounding errors
x=X

# r Jags inputs for Middle Mainstem group
jags.data = list('Y','a.min','a.max','A','nRyrs','S.cv','S.obs','C.cv','C.obs',
                 'x','n','bpmu','bptau')

jags.parms = c("R", "N", "S", "U", "alpha", "beta", "lnalpha", "phi", "C", "log.resid",
               "log.resid.0","sigma.R", "lnalpha.c", "mean.log.R0", "pi", "q", 
               "mean.R0", "sigma.R0","S.msy", "S.max", "S.eq", "U.msy", "gamma", 
               "D.sum", "p","log.S","beta.prior")
```

```{r Fit Model for Middle Mainstem group, echo=FALSE, eval=FALSE}
ptm = proc.time()

jagsfit.Ym <- jags.parallel(data=jags.data,  parameters.to.save=jags.parms,n.thin=5,
                            n.iter=300000, model.file=modelFilename,n.burnin = 50000,n.chains=3)

endtime = proc.time()-ptm
#endtime[3]/60
```


```{r middle mainstem posterior editing, echo=FALSE}

# post.Ym = as.mcmc(jagsfit.Ym)
# rm(jagsfit.Ym)

mypost.Ym = as.matrix(post.Ym, chain=F)
rm(post.Ym)

# write.csv(mypost.Ym, paste("midmainstem_posteriors_",sys_time,".csv", sep=""), row.names = FALSE)



R = post.summ(mypost.Ym,"R[")
S = post.summ(mypost.Ym, "S[")
N = post.summ(mypost.Ym, "N[")
U = post.summ(mypost.Ym, "U[")
resid = post.summ(mypost.Ym, "log.resid[")
PP = post.summ(mypost.Ym, "pi[")
alpha = post.summ(mypost.Ym, "alpha")
beta = post.summ(mypost.Ym, "beta")
phi = post.summ(mypost.Ym, "phi")
sigma = post.summ(mypost.Ym, "sigma.R")
S.msy = post.summ(mypost.Ym, "S.msy")
S.eq = post.summ(mypost.Ym, "S.eq")
S.max = post.summ(mypost.Ym, "S.max")
U.msy = post.summ(mypost.Ym, "U.msy")


output_summary_Ym <- cbind(R, S, N, U, resid, PP, alpha, beta, phi, sigma, S.msy, S.max, U.msy) %>%
  as.data.frame()
output_summary_Ym$region <- rep(c("Middle Mainstem"), (5))

# subsample alpha and beta
alphabeta_Ym <- mypost.Ym %>%
  as.data.frame() %>%
  select(alpha, beta) %>%
  sample_n(100, replace=TRUE) %>%
  mutate(region = "Middle Mainstem")

# keep U.msy vector
U.msy.Ym <- mypost.Ym[,512] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Middle Mainstem") %>%
  as.data.frame()

# keep alpha vector
alpha.Ym <- mypost.Ym[,1] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Middle Mainstem") %>%
  as.data.frame()

rm(mypost.Ym)
```




```{r Pelly data and model run, echo=FALSE}
# Create Pelly data frames for data: age, esc, harv
# create input variables for Bayesian model run

age.YP <- age %>%
  filter(region == "Pelly")
esc.YP <- esc %>%
  filter(region == "Pelly") %>%
  mutate(spawn = esc_stock/1000)
harv.YP <- harv %>%
  filter(region == "Pelly") %>%
  mutate(harvest = harv_stock/1000)

# mean escapement over time series
SMAX <- mean(esc.YP$spawn) 

# turn SMAX into beta prior
bpmu = 1/SMAX

# set CV on prior
bptau = 1/((1)^2)

Y = nrow(age.YP)          # number of calendar years observed
a.min = 4                 # minimum age class in data set
a.max = 7                 # maximum age class in data set
A = a.max - a.min + 1     # number of age classes
nRyrs = Y + A - 1         # number of recruitment years (see model code for details)
years = age.YP[,"year"]

# escapement: assume a 30% observation CV if directly observed, 50% otherwise
S.cv = ifelse(esc.YP[,5] == 1, 0.3, 0.5)
S.obs = esc.YP[,6]

# harvest: assume a 15% observation CV if directly observed, 30% otherwise
C.cv = ifelse(harv.YP[,5] == 1, 0.15, 0.30)
C.obs = harv.YP[,6]

# age composition: assume a ESS of 100 if directly observed, 25 otherwise
ESS = ifelse(age.YP$truecount == 1, 100, 25) 
X <- age.YP %>%
  select(3:6)
X <- as.matrix(X)
X = t(apply(X, 1, function(x) x/sum(x)))
X = round(apply(X, 2, function(x) x * ESS))
colnames(X) = NULL
n = rowSums(X)        # n is slightly different than ESS because of rounding errors
x=X

# r Jags inputs for Pelly group
jags.data = list('Y','a.min','a.max','A','nRyrs','S.cv','S.obs','C.cv','C.obs',
                 'x','n','bpmu','bptau')

jags.parms = c("R", "N", "S", "U", "alpha", "beta", "lnalpha", "phi", "C", "log.resid",
               "log.resid.0","sigma.R", "lnalpha.c", "mean.log.R0", "pi", "q", 
               "mean.R0", "sigma.R0","S.msy", "S.max", "S.eq", "U.msy", "gamma", 
               "D.sum", "p","log.S","beta.prior")
```


```{r Fit Model for Pelly group, echo=FALSE, eval=FALSE}
ptm = proc.time()

jagsfit.YP <- jags.parallel(data=jags.data,  parameters.to.save=jags.parms,n.thin=5,
                            n.iter=300000, model.file=modelFilename,n.burnin = 50000,n.chains=3)

endtime = proc.time()-ptm
#endtime[3]/60
```


```{r Pelly posterior editing, echo=FALSE}

# post.YP = as.mcmc(jagsfit.YP)
# rm(jagsfit.YP)

mypost.YP = as.matrix(post.YP, chain=F)
rm(post.YP)


# write.csv(mypost.YP, paste("pelly_posteriors_",sys_time,".csv", sep=""), row.names = FALSE)



R = post.summ(mypost.YP,"R[")
S = post.summ(mypost.YP, "S[")
N = post.summ(mypost.YP, "N[")
U = post.summ(mypost.YP, "U[")
resid = post.summ(mypost.YP, "log.resid[")
PP = post.summ(mypost.YP, "pi[")
alpha = post.summ(mypost.YP, "alpha")
beta = post.summ(mypost.YP, "beta")
phi = post.summ(mypost.YP, "phi")
sigma = post.summ(mypost.YP, "sigma.R")
S.msy = post.summ(mypost.YP, "S.msy")
S.eq = post.summ(mypost.YP, "S.eq")
S.max = post.summ(mypost.YP, "S.max")
U.msy = post.summ(mypost.YP, "U.msy")


output_summary_YP <- cbind(R, S, N, U, resid, PP, alpha, beta, phi, sigma, S.msy, S.max, U.msy) %>%
  as.data.frame()
output_summary_YP$region <- rep(c("Pelly"), (5))

# subsample alpha and beta
alphabeta_YP <- mypost.YP %>%
  as.data.frame() %>%
  select(alpha, beta) %>%
  sample_n(100, replace=TRUE) %>%
  mutate(region = "Pelly")

# keep U.msy vector
U.msy.YP <- mypost.YP[,512] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Pelly") %>%
  as.data.frame()

# keep alpha vector
alpha.YP <- mypost.YP[,1] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Pelly") %>%
  as.data.frame()

rm(mypost.YP)
```





```{r Stewart data and model run, echo=FALSE}
# Create Stewart data frames for data: age, esc, harv
# create input variables for Bayesian model run

age.YS <- age %>%
  filter(region == "Stewart")
esc.YS <- esc %>%
  filter(region == "Stewart") %>%
  mutate(spawn = esc_stock/1000)
harv.YS <- harv %>%
  filter(region == "Stewart") %>%
  mutate(harvest = harv_stock/1000)

# mean escapement over time series
SMAX <- mean(esc.YS$spawn) 

# turn SMAX into beta prior
bpmu = 1/SMAX

# set CV on prior
bptau = 1/((1)^2)

Y = nrow(age.YS)          # number of calendar years observed
a.min = 4                 # minimum age class in data set
a.max = 7                 # maximum age class in data set
A = a.max - a.min + 1     # number of age classes
nRyrs = Y + A - 1         # number of recruitment years (see model code for details)
years = age.YS[,"year"]

# escapement: assume a 30% observation CV if directly observed, 50% otherwise
S.cv = ifelse(esc.YS[,5] == 1, 0.3, 0.5)
S.obs = esc.YS[,6]

# harvest: assume a 15% observation CV if directly observed, 30% otherwise
C.cv = ifelse(harv.YS[,5] == 1, 0.15, 0.30)
C.obs = harv.YS[,6]

# age composition: assume a ESS of 100 if directly observed, 25 otherwise
ESS = ifelse(age.YS$truecount == 1, 100, 25) 
X <- age.YS %>%
  select(3:6)
X <- as.matrix(X)
X = t(apply(X, 1, function(x) x/sum(x)))
X = round(apply(X, 2, function(x) x * ESS))
colnames(X) = NULL
n = rowSums(X)        # n is slightly different than ESS because of rounding errors
x=X

# r Jags inputs for Stewart group
jags.data = list('Y','a.min','a.max','A','nRyrs','S.cv','S.obs','C.cv','C.obs',
                 'x','n','bpmu','bptau')

jags.parms = c("R", "N", "S", "U", "alpha", "beta", "lnalpha", "phi", "C", "log.resid",
               "log.resid.0","sigma.R", "lnalpha.c", "mean.log.R0", "pi", "q", 
               "mean.R0", "sigma.R0","S.msy", "S.max", "S.eq", "U.msy", "gamma", 
               "D.sum", "p","log.S","beta.prior")
```


```{r Fit Model for Stewart group, echo=FALSE, eval=FALSE}
ptm = proc.time()

jagsfit.YS <- jags.parallel(data=jags.data,  parameters.to.save=jags.parms,n.thin=5,
                            n.iter=300000, model.file=modelFilename,n.burnin = 50000,n.chains=3)

endtime = proc.time()-ptm
#endtime[3]/60
```



```{r Stewart posterior editing, echo=FALSE}

# post.YS = as.mcmc(jagsfit.YS)
# rm(jagsfit.YS)

mypost.YS = as.matrix(post.YS, chain=F)
rm(post.YS)


# write.csv(mypost.YS, paste("stewart_posteriors_",sys_time,".csv", sep=""), row.names = FALSE)


R = post.summ(mypost.YS,"R[")
S = post.summ(mypost.YS, "S[")
N = post.summ(mypost.YS, "N[")
U = post.summ(mypost.YS, "U[")
resid = post.summ(mypost.YS, "log.resid[")
PP = post.summ(mypost.YS, "pi[")
alpha = post.summ(mypost.YS, "alpha")
beta = post.summ(mypost.YS, "beta")
phi = post.summ(mypost.YS, "phi")
sigma = post.summ(mypost.YS, "sigma.R")
S.msy = post.summ(mypost.YS, "S.msy")
S.eq = post.summ(mypost.YS, "S.eq")
S.max = post.summ(mypost.YS, "S.max")
U.msy = post.summ(mypost.YS, "U.msy")


output_summary_YS <- cbind(R, S, N, U, resid, PP, alpha, beta, phi, sigma, S.msy, S.max, U.msy) %>%
  as.data.frame()
output_summary_YS$region <- rep(c("Stewart"), (5))

# subsample alpha and beta
alphabeta_YS <- mypost.YS %>%
  as.data.frame() %>%
  select(alpha, beta) %>%
  sample_n(100, replace=TRUE) %>%
  mutate(region = "Stewart")

# keep U.msy vector
U.msy.YS <- mypost.YS[,512] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Stewart") %>%
  as.data.frame()

# keep alpha vector
alpha.YS <- mypost.YS[,1] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Stewart") %>%
  as.data.frame()

rm(mypost.YS)
```





```{r Teslin data and model run, echo=FALSE}
# Create Teslin data frames for data: age, esc, harv
# create input variables for Bayesian model run

age.YT <- age %>%
  filter(region == "Teslin River")

esc.YT <- esc %>%
  filter(region == "Teslin River") %>%
  mutate(spawn = esc_stock/1000)

harv.YT <- harv %>%
  filter(region == "Teslin River") %>%
  mutate(harvest = harv_stock/1000)

# mean escapement over time series
SMAX <- mean(esc.YT$spawn) 

# turn SMAX into beta prior
bpmu = 1/SMAX


# set CV on prior
bptau = 1/((1)^2)

Y = nrow(age.YT)          # number of calendar years observed
a.min = 4                 # minimum age class in data set
a.max = 7                 # maximum age class in data set
A = a.max - a.min + 1     # number of age classes
nRyrs = Y + A - 1         # number of recruitment years (see model code for details)
years = age.YT[,"year"]

# escapement: assume a 30% observation CV if directly observed, 50% otherwise
S.cv = ifelse(esc.YT[,5] == 1, 0.3, 0.5)
S.obs = esc.YT[,6]

# harvest: assume a 15% observation CV if directly observed, 30% otherwise
C.cv = ifelse(harv.YT[,5] == 1, 0.15, 0.30)
C.obs = harv.YT[,6]

# age composition: assume a ESS of 100 if directly observed, 25 otherwise
ESS = ifelse(age.YT$truecount == 1, 100, 25) 
X <- age.YT %>%
  select(3:6)
X <- as.matrix(X)
X = t(apply(X, 1, function(x) x/sum(x)))
X = round(apply(X, 2, function(x) x * ESS))
colnames(X) = NULL
n = rowSums(X)        # n is slightly different than ESS because of rounding errors
x=X

# r Jags inputs for Teslin group
jags.data = list('Y','a.min','a.max','A','nRyrs','S.cv','S.obs','C.cv','C.obs',
                 'x','n','bpmu','bptau')

jags.parms = c("R", "N", "S", "U", "alpha", "beta", "lnalpha", "phi", "C", "log.resid",
               "log.resid.0","sigma.R", "lnalpha.c", "mean.log.R0", "pi", "q", 
               "mean.R0", "sigma.R0","S.msy", "S.max", "S.eq", "U.msy", "gamma", 
               "D.sum", "p","log.S","beta.prior")
```

```{r Fit Model for Teslin group, echo=FALSE, eval=FALSE}
ptm = proc.time()

jagsfit.YT <- jags.parallel(data=jags.data,  parameters.to.save=jags.parms,n.thin=5,
                            n.iter=300000, model.file=modelFilename,n.burnin = 50000,n.chains=3)

endtime = proc.time()-ptm
endtime[3]/60
```


```{r Teslin River posterior editing, echo=FALSE}

# post.YT = as.mcmc(jagsfit.YT)
# rm(jagsfit.YT)

mypost.YT = as.matrix(post.YT, chain=F)
rm(post.YT)


# write.csv(mypost.YT, paste("teslin_posteriors_",sys_time,".csv", sep=""), row.names = FALSE)


R = post.summ(mypost.YT,"R[")
S = post.summ(mypost.YT, "S[")
N = post.summ(mypost.YT, "N[")
U = post.summ(mypost.YT, "U[")
resid = post.summ(mypost.YT, "log.resid[")
PP = post.summ(mypost.YT, "pi[")
alpha = post.summ(mypost.YT, "alpha")
beta = post.summ(mypost.YT, "beta")
phi = post.summ(mypost.YT, "phi")
sigma = post.summ(mypost.YT, "sigma.R")
S.msy = post.summ(mypost.YT, "S.msy")
S.eq = post.summ(mypost.YT, "S.eq")
S.max = post.summ(mypost.YT, "S.max")
U.msy = post.summ(mypost.YT, "U.msy")


output_summary_YT <- cbind(R, S, N, U, resid, PP, alpha, beta, phi, sigma, S.msy, S.max, U.msy) %>%
  as.data.frame()
output_summary_YT$region <- rep(c("Teslin River"), (5))

# subsample alpha and beta
alphabeta_YT <- mypost.YT %>%
  as.data.frame() %>%
  select(alpha, beta) %>%
  sample_n(100, replace=TRUE) %>%
  mutate(region = "Teslin River")

# keep U.msy vector
U.msy.YT <- mypost.YT[,512] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Teslin River") %>%
  as.data.frame()

# keep alpha vector
alpha.YT <- mypost.YT[,1] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Teslin River") %>%
  as.data.frame()

rm(mypost.YT)
```



```{r Upper Lakes and Mainstem data and model run, echo=FALSE}
# Create Upper Lakes and Mainstem data frames for data: age, esc, harv
# create input variables for Bayesian model run

age.Yu <- age %>%
  filter(region == "Upper Lakes and Mainstem")
esc.Yu <- esc %>%
  filter(region == "Upper Lakes and Mainstem") %>%
  mutate(spawn = esc_stock/1000)
harv.Yu <- harv %>%
  filter(region == "Upper Lakes and Mainstem") %>%
  mutate(harvest = harv_stock/1000)

# mean escapement over time series
SMAX <- mean(esc.Yu$spawn) 

# turn SMAX into beta prior
bpmu = 1/SMAX

# set CV on prior
bptau = 1/((1)^2)

Y = nrow(age.Yu)          # number of calendar years observed
a.min = 4                 # minimum age class in data set
a.max = 7                 # maximum age class in data set
A = a.max - a.min + 1     # number of age classes
nRyrs = Y + A - 1         # number of recruitment years (see model code for details)
years = age.Yu[,"year"]

# escapement: assume a 30% observation CV if directly observed, 50% otherwise
S.cv = ifelse(esc.Yu[,5] == 1, 0.3, 0.5)
S.obs = esc.Yu[,6]

# harvest: assume a 15% observation CV if directly observed, 30% otherwise
C.cv = ifelse(harv.Yu[,5] == 1, 0.15, 0.30)
C.obs = harv.Yu[,6]

# age composition: assume a ESS of 100 if directly observed, 25 otherwise
ESS = ifelse(age.Yu$truecount == 1, 100, 25) 
X <- age.Yu %>%
  select(3:6)
X <- as.matrix(X)
X = t(apply(X, 1, function(x) x/sum(x)))
X = round(apply(X, 2, function(x) x * ESS))
colnames(X) = NULL
n = rowSums(X)        # n is slightly different than ESS because of rounding errors
x=X

# r Jags inputs for Upper Lakes and Mainstem group
jags.data = list('Y','a.min','a.max','A','nRyrs','S.cv','S.obs','C.cv','C.obs',
                 'x','n','bpmu','bptau')

jags.parms = c("R", "N", "S", "U", "alpha", "beta", "lnalpha", "phi", "C", "log.resid",
               "log.resid.0","sigma.R", "lnalpha.c", "mean.log.R0", "pi", "q", 
               "mean.R0", "sigma.R0","S.msy", "S.max", "S.eq", "U.msy", "gamma", 
               "D.sum", "p","log.S","beta.prior")
```

```{r Fit Model for Upper Lakes and Mainstem group, echo=FALSE, cache=TRUE, eval=FALSE}
ptm = proc.time()

jagsfit.Yu <- jags.parallel(data=jags.data,  parameters.to.save=jags.parms,n.thin=5,
                            n.iter=300000, model.file=modelFilename,n.burnin = 50000,n.chains=3)

endtime = proc.time()-ptm
#endtime[3]/60
```


```{r upper lakes and mainstem posterior editing, echo=FALSE, cache=TRUE}

# post.Yu = as.mcmc(jagsfit.Yu)
# rm(jagsfit.Yu)

mypost.Yu = as.matrix(post.Yu, chain=F)
rm(post.Yu)

# write.csv(mypost.Yu, paste("upper_posteriors_",sys_time,".csv", sep=""), row.names = FALSE)



R = post.summ(mypost.Yu,"R[")
S = post.summ(mypost.Yu, "S[")
N = post.summ(mypost.Yu, "N[")
U = post.summ(mypost.Yu, "U[")
resid = post.summ(mypost.Yu, "log.resid[")
PP = post.summ(mypost.Yu, "pi[")
alpha = post.summ(mypost.Yu, "alpha")
beta = post.summ(mypost.Yu, "beta")
phi = post.summ(mypost.Yu, "phi")
sigma = post.summ(mypost.Yu, "sigma.R")
S.msy = post.summ(mypost.Yu, "S.msy")
S.eq = post.summ(mypost.Yu, "S.eq")
S.max = post.summ(mypost.Yu, "S.max")
U.msy = post.summ(mypost.Yu, "U.msy")


output_summary_Yu <- cbind(R, S, N, U, resid, PP, alpha, beta, phi, sigma, S.msy, S.max, U.msy) %>%
  as.data.frame()
output_summary_Yu$region <- rep(c("Upper Lakes and Mainstem"), (5))

# subsample alpha and beta
alphabeta_Yu <- mypost.Yu %>%
  as.data.frame() %>%
  select(alpha, beta) %>%
  sample_n(100, replace=TRUE) %>%
  mutate(region = "Upper Lakes and Mainstem")

# keep U.msy vector
U.msy.Yu <- mypost.Yu[,512] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Upper Lakes and Mainstem") %>%
  as.data.frame()

# keep alpha vector
alpha.Yu <- mypost.Yu[,1] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "Upper Lakes and Mainstem") %>%
  as.data.frame()

rm(mypost.Yu)
```



```{r White-Donjek data and model run, echo=FALSE}
# Create White-Donjek data frames for data: age, esc, harv
# create input variables for Bayesian model run

age.YWD <- age %>%
  filter(region == "White-Donjek")
esc.YWD <- esc %>%
  filter(region == "White-Donjek") %>%
  mutate(spawn = case_when(esc_stock > 0 ~ esc_stock/1000,
                           esc_stock == 0 ~ 0.00001)) # change 0 value to 0.0001 so model will run
  
harv.YWD <- harv %>%
  filter(region == "White-Donjek") %>%
  mutate(harvest = case_when(harv_stock > 0 ~ harv_stock/1000,
                           harv_stock == 0 ~ 0.00001)) # change 0 value to 0.0001 so model will run

# mean escapement over time series
SMAX <- mean(esc.YWD$spawn) 

# turn SMAX into beta prior
bpmu = 1/SMAX

# set CV on prior
bptau = 1/((1)^2)

Y = nrow(age.YWD)          # number of calendar years observed
a.min = 4                 # minimum age class in data set
a.max = 7                 # maximum age class in data set
A = a.max - a.min + 1     # number of age classes
nRyrs = Y + A - 1         # number of recruitment years (see model code for details)
years = age.YWD[,"year"]

# escapement: assume a 30% observation CV if directly observed, 50% otherwise
S.cv = ifelse(esc.YWD[,5] == 1, 0.3, 0.5)
S.obs = esc.YWD[,6]

# harvest: assume a 15% observation CV if directly observed, 30% otherwise
C.cv = ifelse(harv.YWD[,5] == 1, 0.15, 0.30)
C.obs = harv.YWD[,6]

# age composition: assume a ESS of 100 if directly observed, 25 otherwise
ESS = ifelse(age.YWD$truecount == 1, 100, 25) 
X <- age.YWD %>%
  select(3:6)
X <- as.matrix(X)
X = t(apply(X, 1, function(x) x/sum(x)))
X = round(apply(X, 2, function(x) x * ESS))
colnames(X) = NULL
n = rowSums(X)        # n is slightly different than ESS because of rounding errors
x=X

# r Jags inputs for White-Donjek group
jags.data = list('Y','a.min','a.max','A','nRyrs','S.cv','S.obs','C.cv','C.obs',
                 'x','n','bpmu','bptau')

jags.parms = c("R", "N", "S", "U", "alpha", "beta", "lnalpha", "phi", "C", "log.resid",
               "log.resid.0","sigma.R", "lnalpha.c", "mean.log.R0", "pi", "q", 
               "mean.R0", "sigma.R0","S.msy", "S.max", "S.eq", "U.msy", "gamma", 
               "D.sum", "p","log.S","beta.prior")
```

```{r Fit Model for White-Donjek group, echo=FALSE, cache=TRUE, eval=FALSE}
ptm = proc.time()

jagsfit.YWD <- jags.parallel(data=jags.data,  parameters.to.save=jags.parms,n.thin=5,
                            n.iter=300000, model.file=modelFilename,n.burnin = 50000,n.chains=3)

endtime = proc.time()-ptm
endtime[3]/60
```

```{r white-donjek posterior editing, echo=FALSE, cache=TRUE}

# post.YWD = as.mcmc(jagsfit.YWD)
# rm(jagsfit.YWD)

mypost.YWD = as.matrix(post.YWD, chain=F)
rm(post.YWD)


# write.csv(mypost.YWD, paste("whitedonjek_posteriors_",sys_time,".csv", sep=""), row.names = FALSE)



R = post.summ(mypost.YWD,"R[")
S = post.summ(mypost.YWD, "S[")
N = post.summ(mypost.YWD, "N[")
U = post.summ(mypost.YWD, "U[")
resid = post.summ(mypost.YWD, "log.resid[")
PP = post.summ(mypost.YWD, "pi[")
alpha = post.summ(mypost.YWD, "alpha")
beta = post.summ(mypost.YWD, "beta")
phi = post.summ(mypost.YWD, "phi")
sigma = post.summ(mypost.YWD, "sigma.R")
S.msy = post.summ(mypost.YWD, "S.msy")
S.eq = post.summ(mypost.YWD, "S.eq")
S.max = post.summ(mypost.YWD, "S.max")
U.msy = post.summ(mypost.YWD, "U.msy")


output_summary_YWD <- cbind(R, S, N, U, resid, PP, alpha, beta, phi, sigma, S.msy, S.max, U.msy) %>%
  as.data.frame()
output_summary_YWD$region <- rep(c("White-Donjek"), (5))

# subsample alpha and beta
alphabeta_YWD <- mypost.YWD %>%
  as.data.frame() %>%
  select(alpha, beta) %>%
  sample_n(100, replace=TRUE) %>%
  mutate(region = "White-Donjek")

# keep U.msy vector
U.msy.YWD <- mypost.YWD[,512] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "White-Donjek") %>%
  as.data.frame()

# keep alpha vector
alpha.YWD <- mypost.YWD[,1] %>%
  as.data.frame() %>%
  sample_n(1000, replace=TRUE) %>%
  mutate(region = "White-Donjek") %>%
  as.data.frame()

rm(mypost.YWD)
```



```{r create dataframe of combined substock posterior values, echo=FALSE, cache=TRUE}

# merge alpha/beta subsample dataframes

alpha_beta <- do.call("rbind", list(alphabeta_ag,alphabeta_YC,alphabeta_YlC,alphabeta_Ym,alphabeta_YP,
                                    alphabeta_YS,alphabeta_YT,alphabeta_Yu,alphabeta_YWD))


combined_summary <- do.call("rbind", list(output_summary_ag, output_summary_YC, output_summary_YlC, output_summary_YP, 
                                        output_summary_YS, output_summary_YT, output_summary_Ym, output_summary_Yu, output_summary_YWD))


PARAMS_OUTPUT <- combined_summary %>%
  dplyr::select(181,                                           # region
                1,12,23,32:37,2:11,13:22,24:31,                # R
                38,49,60,66:71,39:48,50:59,61:65,              # S
                72,83,94,100:105,73:82,84:93,95:99,            # N 
                106,117,128,134:139,107:116,118:127,129:133,   # U
                140,151,162,164:169,141:150,152:161,163,       # log.resid
                170:179)                                       # other params

log_resid <- combined_summary %>%
  dplyr::select(181,140,151,162,164:169,141:150,152:161,163)

alpha_df <- do.call("rbind", list(alpha.ag, alpha.YC, alpha.YlC, alpha.Ym, alpha.YP,
                                  alpha.YS, alpha.YT, alpha.Yu, alpha.YWD)) %>%
  rename(alpha = ".")


U.msy_df <- do.call("rbind", list(U.msy.ag, U.msy.YC, U.msy.YlC, U.msy.Ym, U.msy.YP, U.msy.YS, U.msy.YT, U.msy.Yu, U.msy.YWD)) %>%
  rename(Umsy = ".") 

#sys_time <- format(Sys.time(),"%Y-%m-%d")
#write.csv(mypost.ag, 'posterior outputs/paste("aggregate_posteriors_",sys_time,".csv", sep="")')
```



```{r Figure 4 aggregate time series stock, echo=FALSE, warning=FALSE, fig.align='center', fig.dim=c(5,5), fig.cap="*Figure 4*. Time-series of aggregate Canadian spawner abundance, estimated harvest and corresponding harvest rate."}

# merge spawners and harvest data frame
w <- merge(esc.ag, harv.ag) %>%
  select(year, spawn, harvest) %>%
  gather(Stock, value, spawn:harvest) %>%
  as.data.frame()

er <- er %>% filter(year <= 2015) %>% as.data.frame()

ggplot(w,aes(x=as.factor(year),y=value, fill = Stock)) +
  geom_bar(stat = "identity", width=0.6, position = position_stack(reverse = FALSE)) +
  geom_line(data=er, mapping = aes(x=as.factor(year), y=er*300), color="red",stat="identity",inherit.aes = FALSE, group=1) +
  xlab("Year") +
  ylab("Count (000s)") +
  scale_fill_manual(values = c("grey76", "grey28")) +
  scale_y_continuous(limits=c(0,300), breaks=c(0,50,100,150,200, 250, 300),
                     sec.axis = sec_axis(~ . /3 , name = "Aggregate harvest rate (%)", 
                                         breaks = seq(0,100,20), labels=seq(0,100,20))) +
  scale_x_discrete(breaks = c(1982,1984,1986,1988,1990,1992,1994,1996,1998,2000,2002,2004,2006,2008,2010,2012,2014)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle=45,hjust=1, size=10)) +
  theme(axis.text.y = element_text(size=10)) +
  theme(legend.justification = "center") +
  theme(axis.title.y.right = element_text(color="red"))
```





```{r Figure 5 substock time series stock, echo=FALSE, warning=FALSE, fig.align='center', fig.dim=c(5,5), fig.cap= "*Figure 5*. Time-series of spawner abundance and total harvest abundance by sub-stock. Years with missing composition data (1984, 1988-1990, 1998, 2006-2007) were ‘infilled’ (i.e., the sub-stock proportions were estimated by taking the average of the values from the two closest years of data."}


fig5_data <- merge(esc,harv) %>%
  rename(harvest = harv_stock) %>%
  rename(spawn = esc_stock) %>%
  select(year, region, harvest, spawn) %>%
  gather(Stock, value, spawn:harvest) %>%
  as.data.frame()

ggplot(fig5_data,aes(x=as.factor(year),y=value/1000, fill = Stock)) +
  geom_bar(stat = "identity", width=0.6, position = position_stack(reverse = FALSE)) +
  xlab("Year") +
  ylab("Count (000s)") +
  scale_fill_manual(values = c("grey76", "grey28")) +
  scale_x_discrete(breaks = c(1982,1986,1990,1994,1998,2002,2006,2010,2014)) +
  facet_wrap(~region, nrow= 3, scales = "free_y") +
  theme_bw() +
  theme(axis.text.x = element_text(angle=45,hjust=1, size=10)) +
  theme(axis.text.y = element_text(size=10)) +
  theme(legend.justification = "center")
```




```{r get data ready for Figure 6}

# head(alpha_beta)

# get range of abundances for stock; values are recorded in sp_ag vector below
# don't need to re-run
esc.range.sub <- esc %>%
  group_by(region) %>%
  mutate(min = min(esc_stock)/1000) %>%
  mutate(max = max(esc_stock)/1000) %>%
  distinct(region, min, max)


esc.range.ag <- esc.ag %>%
  mutate(min = min(spawn)) %>%
  mutate(max = max(spawn)) %>%
  mutate(region = "aggregate") %>%
  distinct(region, min, max)
  
esc.range <- full_join(esc.range.sub, esc.range.ag)

#  alpha and beta
alpha_beta2 <- alpha_beta[rep(seq_len(nrow(alpha_beta)),each=100),]

# create vector of abundances for each dataframe
sp_ag <- as.vector(seq(0,300,length.out=100))
sp_YC <- as.vector(seq(0,25,length.out=100))
sp_YlC <- as.vector(seq(0,7.5,length.out=100))
sp_Ym <- as.vector(seq(0,25,length.out=100))
sp_YP <- as.vector(seq(0,20,length.out=100))
sp_YS <- as.vector(seq(0,90,length.out=100))
sp_YT <- as.vector(seq(0,100,length.out=100))
sp_Yu <- as.vector(seq(0,15,length.out=100))
sp_YWD <- as.vector(seq(0,11,length.out=100))

# repeat substock abundance vector 100 times for each value of alpha and beta
spw_ag <- rep(c(sp_ag),(100))
spw_YC <- rep(c(sp_YC),(100))
spw_YlC <- rep(c(sp_YlC),(100))
spw_Ym <- rep(c(sp_Ym),(100))
spw_YP <- rep(c(sp_YP),(100))
spw_YS <- rep(c(sp_YS),(100))
spw_YT <- rep(c(sp_YT),(100)) 
spw_Yu <- rep(c(sp_Yu),(100))
spw_YWD <- rep(c(sp_YWD),(100))

# combine abundances into single vector
spw_df <- c(spw_ag,spw_YC,spw_YlC,spw_Ym,spw_YP,spw_YS,spw_YT,spw_Yu,spw_YWD)

# add column of abundances to alphabeta_df2 dataframe
alpha_beta2$abund <- NA
alpha_beta2$abund <- spw_df


# add on 'y' value that has the predicted ricker estimates per abundance value
alpha_beta2$y <- NA
alpha_beta2$y <- alpha_beta2$alpha*alpha_beta2$abund*exp(-alpha_beta2$beta*alpha_beta2$abund)

# add scenario simulation number
scen <- rep(1:100, each = 100)
alpha_beta2$scenario <- NA
alpha_beta2$scenario <- rep(c(scen),c(1))


med_lwr_upp_PARAMS <- PARAMS_OUTPUT[c(3:5,8:10,13:15,18:20,23:25,28:30,33:35,38:40,43:45),]

reg_vec <- rep(c("aggregate", "Carmacks", "Lower Mainstem", "Middle Mainstem", "Pelly", "Stewart", "Teslin River",
                 "Upper Lakes and Mainstem", "White-Donjek"), each = 3)

thresh_vec <- rep(c("median", "lower", "upper"), times = 9)

R_df <- med_lwr_upp_PARAMS[1:38] %>%
  mutate(region = reg_vec) %>%
  mutate(threshold = thresh_vec) %>%
  dplyr::select(1,39,2:38)

S_df <- med_lwr_upp_PARAMS[,c(1,39:72)] %>%
  mutate(region = reg_vec) %>%
  mutate(threshold = thresh_vec) %>%
  dplyr::select(1,36,2:35)


# merge R and S substock dataframes
R_S_df <- cbind(R_df, S_df)

year_vec1 <- rep(c(1:37),each=27)
year_vec2 <- rep(c(1:34), each=27)
year_vec <- c(year_vec1, year_vec2)

parameterR <- rep(c("R"), times=999)
parameterS <- rep(c("S"), times=918)
param_vec <- c(parameterR, parameterS)


R_S_df2 <- R_S_df[,c(1:39,42:75)] %>%
  gather(key = param, value = value, "R[1]":"S[34]") %>%
  mutate(year = year_vec) %>%
  mutate(parameter = param_vec) %>%
  dplyr::select(region, year, threshold, parameter, value) %>%
  spread(key=parameter, value=value)
  
R_S_df3 <- R_S_df2 %>%
  nest(R,S, .key = 'value_col') %>%
  spread(key=threshold, value = value_col) %>%
  unnest(lower, median, upper, .sep = '_') %>%
  as.data.frame()
```

```{r plot figure, fig.dim=c(5,5), fig.align='center', warning=FALSE, echo=FALSE}
ggplot(data=R_S_df3, aes(x = median_S, y = median_R, group=region)) +
  geom_line(data=alpha_beta2, aes(x = abund, y = y, group=scenario), colour="gray63") +
  geom_point(size=1) +
  geom_pointrange(aes(ymin = lower_R, ymax = upper_R)) +
  geom_errorbarh(aes(xmin = lower_S, xmax = upper_S)) +
  xlab("Spawners (000s)") +
  ylab("Recruits (000s)") +
  facet_wrap(~region, ncol=3, scales = "free") + #
  theme_bw() +
  theme(axis.title = element_text(size=12)) +
  theme(axis.text = element_text(size=10))
  
```



**Objective 3**
To characterize Chinook population diversity within the CDN portion of the Yukon River we fit individual age-structured state-space stock-recruitment models to all available for each sub-stock (i.e., time series of estimated catch, spawner abundance and age composition). As a starting point we made the simplifying assumption that all sub-stock experienced the same exploitation rates over the time series, and that this exploitation rate was equal to that estimated for the stock-aggregate as a whole (JTC 2017). Similarly, we assumed that age composition was the same across sub-stocks and equal to that estimated for the stock-aggregate as a whole. We used the spawner abundance and harvest estimates reconstructed by sub-stock in Objective 2. We fit these data in a Bayesian estimation framework using an approach that closely follows that of Fleischman et al. (2013) and which allowed for temporal variability in productivity and age-at-maturity. A more complete description of the model and our approach to fitting it is provided in Appendix A.

Some of the CDN-Yukon Chinook sub-stocks were estimated to be very productive (perhaps suspiciously so?), and there was considerable variability among sub-stocks in both productivity and carrying capacity (Figure 7). However, these estimates and the overall shape of the stock recruitment relationships are highly uncertain (Figure 8). It is possible the small sample sizes used to estimate the contributions of small sub-stocks (e.g., Middle Mainstem), particularly during the early portion of the time series when exploitation rates were very high, may result in biased high estimates of recruitment and productivity. This suggests that it might be worthwhile trying to increase the number of scale samples we use for genetic stock ID during the early portion of the time series. 

We found evidence for strong serial correlation in survival (average ϕ = 0.71), and non-stationary productivity (Figure 7B). In addition there was considerable variation among sub-stocks in productivity over time (Figure 6a): sub-stocks in the lower portion of the CDN basin tended to exhibit a period of elevated productivity in the 1980s and early 1990s followed by depressed productivity in the early 2000s (Lower Mainstem, Stewart, Pelly, White-Donjek) while those in the upper portion of the basin (Middle Mainstem, Carmacks, Upper Lakes and Teslin) tended to not exhibit depressed productivity in the early 2000s and have shown signs of above average productivity in recent years. 
The range of sub-stock productivities we estimated correspond to harvest rates predicted to maximize long-tern yield (i.e., UMSY) that range from ~55% to 80% (Figure 8).    


```{r Figure 7: alpha posteriors, fig.dim=c(5,5), fig.align='center'}

# list of colors that matches map colors
# "#ffc431", orange, lower mainstem
# "#ffc7ae", pink, pelly
# "#a9fee0", aquamarine, stewart
# "#dd8071", red, carmacks
# "#f2dc3b", yellow, middle mainstem
# "#cadeae", moss green, upper lakes
# "#288baf", dark blue, white-donjek
# "#d2adea", purple, teslin


# plot productivity residuals
# create color scheme
MyColour <- c("gray63","#ffc431", "#a9fee0", "#ffc7ae", "#288baf", "#f2dc3b", "#dd8071", "#cadeae", "#d2adea")

# order substock factors
alpha_df$reg2 <- factor(alpha_df$region, levels = c("aggregate","Lower Mainstem","Stewart","Pelly","White-Donjek","Middle Mainstem",
                                               "Carmacks", "Upper Lakes and Mainstem","Teslin River"))

ggplot(alpha_df, aes(x=reg2, y=alpha, fill=reg2)) +
  geom_boxplot(outlier.shape = NA, size=1.25) +
  #coord_flip() +
  xlab("Substock") +
  ylab("Alpha posteriors") +
  geom_vline(xintercept = 1.5, lty="dotted") +
  #scale_y_continuous(limits=c(0,1)) +
  scale_fill_manual(values = MyColour) +
  theme_classic() +
  theme(axis.text.x = element_text(angle=45, hjust=1, size=10))+
  theme(axis.text.y = element_text(size=10)) +
  theme(axis.title = element_text(size=12)) +
  theme(legend.position="none")

```


```{r Figure 8: productivity index log residuals, fig.align='center', fig.dim=c(5,5), warning=FALSE}

# head(log_resid)
year_vec <- rep(c(1982:2011), each=45)
stat_vec <- rep(c("mean","sd","median","lower","upper"), times=9)

df <- log_resid %>%
  mutate(stat=stat_vec) %>%
  gather(key = resid, value = value, "log.resid[1]":"log.resid[30]") %>%
  mutate(year = year_vec) %>%
  spread(key = stat, value = value) %>%
  arrange(region, year)

# create color scheme
MyColour <- c("gray63","#ffc431", "#a9fee0", "#ffc7ae", "#288baf", "#f2dc3b", "#dd8071", "#cadeae", "#d2adea")

# order substock factors
df$reg2 <- factor(df$region, levels = c("aggregate","Lower Mainstem","Stewart","Pelly","White-Donjek","Middle Mainstem",
                                               "Carmacks", "Upper Lakes and Mainstem","Teslin River"))

ggplot(df, aes(x=year, y = median, colour = reg2), show.legend = F) +
  geom_point(size=1,show.legend = F) +
  scale_color_manual(values = MyColour) +
  geom_line(show.legend = F) +
  geom_ribbon(aes(ymin=lower, ymax=upper,fill=reg2), show.legend = F, alpha=0.2) +
  scale_fill_manual(values = MyColour) +
  geom_hline(yintercept = 0, lty = "dotted") +     #geom_hline(data=df_full,aes(yintercept = alpha_adj), lty="dotted") +
  xlab("Brood year") +
  ylab("Productivity index") +
  scale_x_continuous(breaks=c(1982, 1986, 1990, 1994, 1998, 2002, 2006, 2010)) +
  scale_y_continuous(limits=c(-5,5), breaks=c(-5,-3,-1,1,3,5)) +
  facet_wrap(~reg2,nrow=4) +  # ,scales = "free_y"
  theme(legend.position = "none") +
  theme_bw() +
  theme(axis.text.x = element_text(angle=45, hjust = 1, size=10))
```

```{r Figure 9: Umsy posteriors, echo=FALSE, fig.align='center', fig.dim=c(5,5), warning=FALSE}


# list of colors that matches map colors
# "#ffc431", orange, lower mainstem
# "#ffc7ae", pink, pelly
# "#a9fee0", aquamarine, stewart
# "#dd8071", red, carmacks
# "#f2dc3b", yellow, middle mainstem
# "#cadeae", moss green, upper lakes
# "#288baf", dark blue, white-donjek
# "#d2adea", purple, teslin


# plot productivity residuals
# create color scheme
MyColour <- c("gray63","#ffc431", "#a9fee0", "#ffc7ae", "#288baf", "#f2dc3b", "#dd8071", "#cadeae", "#d2adea")

# order substock factors
U.msy_df$reg2 <- factor(U.msy_df$region, levels = c("aggregate","Lower Mainstem","Stewart","Pelly","White-Donjek","Middle Mainstem",
                                               "Carmacks", "Upper Lakes and Mainstem","Teslin River"))



ggplot(U.msy_df, aes(x=reg2, y=Umsy, fill=reg2)) +
  geom_boxplot(outlier.shape = NA, size=1.25) +
  #coord_flip() +
  xlab("Substock") +
  ylab("U.msy posteriors") +
  geom_vline(xintercept = 1.5, lty="dotted") +
  scale_y_continuous(limits=c(0,1)) +
  scale_fill_manual(values = MyColour) +
  theme_classic() +
  theme(axis.text.x = element_text(angle=45, hjust=1, size=10))+
  theme(axis.text.y = element_text(size=10)) +
  theme(axis.title = element_text(size=12)) +
  theme(legend.position="none")


```




**Objective 4**
We used the posterior estimates of productivity and carrying capacity to quantify the predicted equilibrium tradeoffs between aggregate harvest and conservation of population diversity across a range of mixed-stock harvest rates (e.g., Walters et al. 2008). The resulting picture illustrates that the relatively high harvest rates that can be sustained by the most productive sub-stocks come at the cost of increased risk to less productive ones (Figure 9). Overall yield from the system is predicted to be maximized at a harvest rate of ~ 60%, but this comes at the cost of overharvesting ~ 40% of the sub-stocks (i.e., harvest rate is > UMSY for a given sub-stock) and putting a small number of the sub-stocks at risk of extirpation). Furthermore, there is clear asymmetry in these tradeoffs where relatively small (8%) reductions in predicted yield (e.g., from 60K to 55K) correspond to relatively large (50%) reductions in biological risk (e.g., from ~40% to 20% overfished). The large uncertainty in our estimates of productivity and carrying capacity result in large uncertainty in these predicted tradeoffs.   



















